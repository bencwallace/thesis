% Parts are the largest structural units, but are optional.
%\part{Thesis}

% Chapters are the next main unit.
\chapter{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical mechanics}

The state of knowledge of a system with state space $(\Omega, d\lambda)$ can be
expressed by a probability measure $\mu$ on $\Omega$. Let $\Mcal_\lambda(\Omega)$ denote
the set of probability measures on $\Omega$ absolutely continuous with respect to $\lambda$.
For $\mu \in \Mcal_\lambda(\Omega)$, we denote the Radon-Nikodym derivative of $\mu$ with
respect to $\lambda$ by $d\mu/d\lambda$ and define the \emph{entropy} of $\mu$ with respect
to $\lambda$ by
\begin{equation}
h(\mu) = h_\lambda(\mu) = -\int_\Omega \log\frac{d\mu}{d\lambda} \; d\mu.
\end{equation}
In many cases, specific information about
the state of knowledge is available, for example in the form of statements such as
\begin{equation}
\int f \; d\mu \in S_f
\end{equation}
with $f$ running over some collection of $\mu$-integrable functions
that represent \emph{observable} quantities of the system, and $S_f$ a Borel
subsets of $\R$ for each $f$. The \emph{principle of maximum entropy} asserts that, in this
case, the measure best expressing the state of knowledge of the system is given by
\begin{equation}
\hat\mu = \argmax(h_\lambda(\mu) : \mu \in \Mcal_\lambda(\Omega)),
\end{equation}
assuming such a measure exists and is unique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Microcanonical ensemble}

Consider an \emph{isolated} physical system on $\Omega$, that is, one that cannot exchange
energy with its surroundings. Such a system can be determined by a choice of function
$H : \Omega \to \R$, called the \emph{Hamiltonian}. The value $H(\omega)$ represents the
total energy of the system in state $\omega\in\Omega$.

\begin{example}
Let $\Omega = U^n \times \R^{3n}$, where $U \subset \R^3$, and denote a generic element of
$\Omega$ by $(q, p)$, where $q \in U^n$ and $p \in \R^{3n}$. Then $\Omega$ is the state
space of a system of $n$ point particles $i = 1, \ldots, n$ with positions $q_i \in U$
and momenta $p_i \in \R^3$. Given a $C^1$ Hamiltonian $H : \Omega \to \R$,
the dynamics of such a system is determined by \emph{Hamilton's equations}
\begin{align}
\dd{q}{t}   &= \nabla_q H(q(t), p(t)) \\
-\dd{p}{t}  &= \nabla_p H(q(t), p(t)).
\end{align}
An immediate consequence of these equations and the chain rule is the \emph{principle
of conservation of energy}:
\begin{equation}
\dd{H}{t}(q(t), p(t)) = 0.
\end{equation}
Thus, a Hamiltonian system with initial configuration $(q(0), p(0))$ of energy
$E = H(q(0), p(0))$ will evolve on the constant energy shell $S_E = H^{-1}(E)$.

% Let us assume that $H$ has compact level sets so that the solutions to Hamilton's
% equations can be extended globally in time. In general, especially when $n$ is large,
% a Hamiltonian system is likely to be too complicated to understand in a precise way.
% The basic idea of statistical mechanics is to leverage this complexity by assuming
% (or in some cases proving) that, as $t\to\infty$, such a system will settle into a
% state of equilibrium in which all possible states are ``equiprobable''.
\end{example}

% Although one can make sense of the notion of an equiprobable probability distribution
% on $S_E$ in the above example, it is simpler (and more relevant to this thesis) to
% consider the case when $S_E$ is finite.
If $S_E = H^{-1}(E)$ is finite\footnote{We can view such a system as an approximation
to a traditional continuous system with $S_E$ uncountable.}, then one can easily determine
the maximum entropy measure on $S_E$. Indeed, the maximum entropy measure on a finite
space is simply the uniform measure. We define the
\emph{microcanonical distribution} on a finite set $F$ to be the uniform
measure on $F$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Canonical ensemble}

In practice, most systems of interest are not truly isolated: they may exchange
energy with their environments. Everyday experience, however, suggests that
a physical system that is left undisturbed for a sufficiently long time will achieve
\emph{thermal equilibrium}, in which the system's temperature is constant and equal to
that of its surroundings. We define the \emph{canonical ensemble} for a system with
state space $\Omega$ and Hamiltonian $H$ on $\Omega$
to be the maximum entropy distribution subject to the fixed average energy constraint
\begin{equation}
\int H \; d\mu = E.
\end{equation}
It can be shown by the method of Lagrange multipliers that the canonical ensemble is
given by the \emph{Gibbs measure}
\begin{equation}
d\mu_\beta = \frac{1}{Z_\beta} e^{-\beta H} d\lambda,
\end{equation}
where
\begin{equation}
Z_\beta = \int e^{-\beta H} \; d\lambda
\end{equation}
is the normalizing constant, known as the \emph{canonical partition function}.
The quantity $\beta = \beta(E)$, which arises as a Lagrange multiplier, is known as
the \emph{inverse temperature}.

The \emph{free energy} of this system is defined by
\begin{equation}
F_\beta = -\frac{1}{\beta} \log Z_\beta.
\end{equation}
This definition may seem obscure at first, but is elucidated by
a computation of the entropy $h_\lambda(\mu_\beta)$, which implies that
\begin{equation}
F_\beta = E - \frac{1}{\beta} h_\lambda(\mu_\beta).
\end{equation}
This is the famous thermodynamic relation between free energy, internal energy $E$,
temperature $1/\beta$, and entropy.


In the context of spin systems, there is a natural mathematical reason for studying measures
of the above form. This is the Hammersley-Clifford theorem, which states that any Markov
random field (a spatial generalization of a Markov chain) on a graph has a representation
as a Gibbs measure whose Hamiltonian is a sum of ``local'' interactions.

% This situation can be modeled as follows: call the system of interest system $A$ and
% its surroundings system $B$. We assume that $B$ behaves as a \emph{thermal reservoir},
% meaning that it is so large that its temperature may be assumed to be effectively
% constant, even if system $A$ is at a different temperature.
% It is then reasonable to assume that the total system $A \times B$ is isolated.

% \begin{example}
% Let $\Omega_1$ and $\Omega_2$ be the state spaces of systems $A$ and
% $B$ and let $H_i : \Omega_i \to \R$ be their respective Hamiltonians.
% The system $A \times B$ then has state space $\Omega_1 \times \Omega_2$.
% If we assume that the contribution to the energy due to interactions between systems $A$
% and $B$ is negligible, then the Hamiltonian of $A \times B$ is the function $H : \Omega \to \R$
% defined by $H(\omega_1, \omega_2) = H_1(\omega_1) + H_2(\omega_2)$.

% Let us assume that the 
% Let $S^i_{E_i} = H_i^{-1}(E_i)$ and $S_E = H^{-1}(S_E)$ and assume that $|S^i_{E_i}| < \infty$
% for any $E_i$. Then
% \begin{equation}
% S_E = \bigcup_{E_1=0}^E S^1_{E_1} \times S^2_{E-E_2}
% \end{equation}
% is finite for all $E$.
% % Suppose that $S_E$ is compact so that the microcanonical ensemble $\mu_E$ on $S_E$ is well-defined.

% For any $E_2 \in \R$, let $S^2_{E_2} = H_2^{-1}(E_2)$. Then the marginal distribution $\mu^1_E$
% of $\mu$ on $\Omega_1$ is given by
% \begin{equation}
% \mu^1_E(\omega_1)
%   =
% \sum_{\omega_2:\omega\in S_E} \mu_E(\omega)
%   =
% \frac{\# S^2_{E-H_1(\omega_1)}}{\# S_E}
%   \propto
% e^{h^2_{E-H_1(\omega_1)}},
% \end{equation}
% where
% \begin{equation}
% h^2_{E_2} = \log |S^2_{E_2}|
% \end{equation}
% is the \emph{entropy} of the microcanonical ensemble on $S^2_{E_2}$.

% If $E_2 \mapsto h^2_{E_2}$ is $C^1$, then we have the first-order expansion
% \begin{equation}
% h^2_{E - H_1(\omega_1)}
%   =
% h^2_E - \dd{h^2_E}{E} H_1(\omega_1) + O(H_1(\omega_1)^2).
% \end{equation}
% Thus,
% \begin{equation}
% e^{-h^2_{E-H_1(\omega_1)}}
%   \approx
% C e^{-\beta H_1(\omega_1)},
% \end{equation}
% where $\beta = \dd{h^2_E}{E}$ and $C$ is a constant independent of $\omega_1$.
% The assumption that system $B$ is a ``heat bath'' then amounts to saying that
% $E - H_1(\omega_1)$ is such a negligible perturbation of $E$ that the first-order
% approximation above accurately represents the marginal density of $\mu^1_E$.
% \end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Grand canonical ensemble}

Consider a physical system that is free to exchange particles with its environment. We suppose
the ``number of particles'' of this system take values in a measurable set $\interval \subset \R$.
For $T \in \interval$, let $\Omega_T$ be the space of configurations with $T$
particles and let $H_T : \Omega_T \to \R$ be the Hamiltonian on $\Omega_T$.
The state space for the full system is then given by $\Omega = \bigsqcup_{T \in \interval} \Omega_T$,
where $\sqcup$ denotes the disjoint union.
For $\omega\in\Omega$, let $|\omega|$ be the unique value of $T$
such that $\omega \in \Omega_T$.
We define $H : \Omega \to \R$ by $H(\omega) = H_{|\omega|}(\omega)$.

The \emph{grand canonical ensemble} for this system is defined to be the maximum entropy measure
subject to the constraints of fixed average energy and fixed average number of particles. It can
be shown that the grand canonical ensemble is given by a measure
\begin{equation}
d\mu_{\beta,\nu} = \frac{1}{Z_{\beta,\nu}} e^{-\beta (H(\omega) - \nu |\omega|)},
\end{equation}
where $Z_{\beta,\nu}$ is the normalizing constant, known as the \emph{grand canonical partition
function}. The parameters $\beta$ and $\nu$ are both Lagrange multipliers; we call $\nu$ the
\emph{fugacity}. Since the $\nu |\omega|$ term can be absorbed into the Hamiltonian, the measure
$\mu_{\beta,\nu}$ is really just an ordinary Gibbs measure as defined in the previous section.
For this reason, the distinction between canonical and grand canonical ensembles is informal.

\begin{example}
Note that the grand canonical partition function is given by
\begin{equation}
Z_{\beta,\nu}
  =
\int_\interval e^{\nu T} Z_\beta^{(T)} \; dT,
\end{equation}
where $Z^{(T)}_\beta$ is the canonical partition function of $H_T$.
Thus, $Z_{\beta,\nu}$ is the Laplace transform of the canonical partition function
viewed as a function of $T$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Graphs}

Most systems of interest do not have a finite (or even countable) state space. Nevertheless,
finite systems serve as natural approximations of real systems. For instance, spatially-extended
systems may be approximated by models on graphs.

\subsubsection{Graphs and the Laplacian}

Let $\vertices$ be a countable set.
Let $\jay$ be a symmetric $\vertices\times\vertices$ matrix satisfying
\begin{equation}
0 < d_x = \sum_{y\in\vertices} J_{xy} < \infty,
  \quad
J_{xy} \ge 0,
  \quad
J_{xx} = 0
\end{equation}
for all $x, y$. Then we can define the diagonal matrix $\diag$ with entries
\begin{equation}
\diag_{xx} = d_x.
\end{equation}
Moreover, if $\edges = \{ \{ x, y \} : J_{xy} \ne 0 \}$, then
$\graph = (\vertices, \edges, \jay)$ is a weighted connected undirected graph.
We write $x \sim y$ if $\{ x, y \} \in \edges$. We will say that $\graph$ is
$d_0$-regular if $d_x = d_0$ for all $x$.

The \emph{graph Laplacian} on $\graph$ is defined by
\begin{equation}
\lap = \diag - \jay.
\end{equation}
We also define the \emph{massive Laplacian} with squared \emph{mass} $m^2 > 0$
by
\begin{equation}
m^2 + \lap.
\end{equation}
Note that
\begin{equation}
\varphi \cdot \lap \varphi
  =
\frac{1}{2} \sum_{x,y\in\vertices} J_{xy} |\varphi_x - \varphi_y|^2
  \ge
0,
\end{equation}
so $\lap$ is positive-semidefinite.

\subsubsection{The Green function}

If $m^2 > 0$, then $m^2 + \lap$ is positive-definite, hence invertible with inverse
\begin{equation}
(m^2 + \lap)^{-1} = (m^2 + D)^{-1} \sum_{n=0}^\infty Z^n P^n,
\end{equation}
where
\begin{align}
Z = (m^2 + D)^{-1} D,
  \quad
P = D^{-1} J.
\end{align}
Let $z_x$ denote the diagonal elements of $Z$.
The \emph{Green function} for $m^2 + \lap$ is the kernel of $(m^2 + \lap)^{-1}$, which we define by
\begin{equation}
C(x, y)
  =
(m^2 + d_x)^{-1} \sum_{n=0}^\infty z_x^n P^n_{xy}
\end{equation}
whenever this series converges.

\begin{example}
An important case is when $\jay$ has $\{0, 1 \}$-valued entries. In this case, $d_x$ is the
\emph{degree} of $x$ in $\graph$ and we denote the matrix $\lap$ by $-\Delta$, which has
entries
\begin{equation}
-\Delta_{xy} = d_x \1_{x=y} - \1_{x \sim y}.
\end{equation}
\end{example}

% Let $\jay$ be a $\vertices \times \vertices$
% matrix with $J_{xx} = 0$ and $J_{xy} \ge 0$ for all $x, y \in \vertices$ and suppose
% that $\jay$ has summable rows: $\sum_{y\in\vertices} J_{xy} < \infty$. Let $\diag$ be
% the diagonal matrix with entries $D_{xx} = \sum_{y\in\vertices} J_{xy}$, and let $A = \diag - \jay$.

% \todo{The $Q$ matrix will be $Q = -A$, the matrix $\jay$ will be the (ferromagnetic) Ising interaction,
% and $A$ will be the interaction for the $|\varphi|^4$ model.
% E.g.\ $A = -\Delta + m^2$ has positive diagonal entries. Recall that
% $\Delta_{xy} = \1_{x \sim y} - 2 d \1_{x=y}$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Spin systems}

An $n$-component \emph{field} or \emph{spin configuration} on $\vertices$
is an element $\varphi$ of $(\R^n)^\vertices$.
We denote the components of a field $\varphi$
by $\varphi^i_x \in \R$ for $x \in \vertices$ and $i = 1, \ldots, n$.
The Euclidean inner product on fields is defined by
\begin{equation}
\varphi\cdot\tilde\varphi
  =
\sum_{x\in\vertices} \varphi_x \cdot \tilde\varphi_y
  =
\sum_{i=1}^n \sum_{x\in\vertices} \varphi^i_x \tilde\varphi^i_x
\end{equation}
and the Euclidean norm is
\begin{equation}
|\varphi|^2 = \varphi \cdot \varphi.
\end{equation}
Given any $\vertices\times\vertices$ matrix $M$, we define the field $M \varphi$ by
\begin{equation}
(M \varphi)_x = \sum_{y\in\vertices} M_{xy} \varphi_y
\end{equation}
when the sum is well-defined.

A \emph{spin system} is just a probability measure $d\mu$ on $\Omega = S^\vertices$.
The elements $\varphi \in \Omega$ are called \emph{fields} or \emph{spin configurations}.

Suppose that $S \subset \R^n$ is endowed with a measure $d\lambda^0$ and let
$H : \Omega \to \R$ be a measurable function.
We wish to study spin systems given by Gibbs measures of the form
\begin{equation}
d\mu_\beta(\varphi) = \frac{1}{Z_\beta} e^{-\beta H(\varphi)} d\lambda(\varphi),
\end{equation}
where
\begin{equation}
d\lambda(\varphi) = \prod_{x\in\vertices} d\lambda^0(\varphi_x).
\end{equation}

However, there are some problems with this definition when $\vertices$ is infinite.
For one, $d\lambda$ may be pathological: for example, if $S = \R$ and $d\lambda^0$
is Lebesgue measure, then it can be shown that $d\lambda$ takes values in $\{0, \infty\}$.
Another problem is that it may be difficult to define a reasonable choice of $H$ on the
infinite product space $\Omega$.

For this reason, we temporarily restrict our attention to finite graphs, and define spin
systems on such graphs as above. In many cases, the Hamiltonian will depend on one or
more parameters; in this case, adjusting $\beta$ is equivalent to rescaling these parameters
and so we will usually drop $\beta$ (or set $\beta = 1$).

We will mainly be concerned with \emph{ferromagnetic} spin systems, for which the Hamiltonian
has the form
\begin{equation}
H(\varphi) = \sum_{x, y \in \vertices} \varphi_x \cdot J_{xy} \varphi_y,
\end{equation}
where (recall) $J_{xy} \ge 0$.
Let us fix a designated vertex\footnote{In general, some quantities we define will depend
on the choice of vertex. However, we will ultimately be interested in the transitive graph
$\graph = \Zd$ for which this choice is irrelevant.} $0 \in \vertices$.
We define the \emph{two-point function} for such a spin system $\mu_\beta$ by
\begin{equation}
G_x(\mu_\beta) = \frac{1}{n} \int d\mu_\beta(\varphi) \; \varphi_0 \cdot \varphi_x
\end{equation}
when this is well-defined. If it is, we also define the \emph{susceptibility}
\begin{equation}
\chi(\mu_\beta) = \sum_{x\in\vertices} G_x(\mu_\beta).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gaussian measures and the free field}

Let $S = \R^n$ and let $C$ be a positive-definite symmetric $\vertices\times\vertices$ matrix.
The $n$-component
\emph{Gaussian measure} $d\mu_C$ on $\vertices$ with mean $0$ and \emph{covariance} $C$ is defined
by the Hamiltonian
\begin{equation}
H_C(\varphi) = \frac{1}{2} \varphi \cdot C^{-1} \varphi.
\end{equation}
This is essentially the simplest choice of non-constant Hamiltonian\footnote{Note that
$e^{-\beta H}$ is not integrable if $H$ is linear.}.
% is the unique measure $\mu$ on $(\R^n)^\vertices$ with characteristic function
% \begin{equation}
% \hat\mu(\xi)
%   \coloneqq
% \int_{(\R^n)^\vertices} \mu(d\varphi) \; e^{i \varphi \cdot \xi}
%   =
% e^{-\frac{1}{2} \xi \cdot C \xi}.
% \end{equation}
% If $\vertices$ is finite, then the Lebesgue measure $d\varphi$ on $(\R^n)^\vertices$
% is well-defined and
The partition function can be computed explicitly, giving\footnote{Here, we have employed
our convention of setting $\beta = 1$ when the Hamiltonian depends on a parameter.}
\begin{equation}
d\mu_C(\varphi)
  =
\frac{1}{\sqrt{\det(2\pi C)}} e^{-\frac{1}{2} \varphi\cdot C^{-1}\varphi} d\varphi.
\end{equation}
By \emph{Wick's theorem}, the two-point function is just the covariance $C$:
\begin{equation}
\int \varphi_a \cdot \varphi_b \; d\mu(\varphi) = C_{ab}.
\end{equation}

An important case is the \emph{Gaussian free field} on $\vertices$ with \emph{mass} $m^2 \ge 0$,
for which the covariance is equal to the massive Green function: $C = (m^2 + \lap)^{-1}$.
It is not hard to show, using the fact that $J$ has nonnegative entries, that the Gaussian
free field is ferromagnetic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The \texorpdfstring{$|\varphi|^4$}{phi4} spin model}

Let $S = \R^n$ again.
The simplest choice of Hamiltonian that is not quadratic is given by a quartic function.
The Hamiltonian for the $|\varphi|^4$ spin model is given by
\begin{equation}
H_{g,\nu}(\varphi)
  =
\sum_{x\in\vertices}
\left(
  \frac{1}{4} g |\varphi_x|^4
    +
  \frac{1}{2} \nu |\varphi_x|^2
    +
  \frac{1}{2} \varphi_x \cdot (\lap \varphi)_x
\right),
\end{equation}
where $g > 0$ and $\nu\in\R$. When $\nu > 0$, we have $H_{g,\nu} = g |\varphi|^4 + H_C$,
where $C = (\nu + \lap)^{-1}$ is the covariance of the Gaussian free field with mass $\nu$.
Again, this is a ferromagnetic spin system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The \texorpdfstring{$O(n)$}{O(n)} spin model}

Let $S = S^{n-1} \subset \R^n$ be the unit $(n-1)$-sphere equipped with the surface measure
$d\lambda^0$ (in particular, $S^0 = \{ \pm 1 \}$, which is equipped with the counting measure).
The $O(n)$ model is defined by the Hamiltonian
\begin{equation}
H_J(\sigma) = -\frac{1}{2} \sigma \cdot J \sigma,
\end{equation}
which is clearly ferromagnetic.

The corresponding Gibbs measure arises naturally as a limiting case of the $|\varphi|^4$
measure $d\mu_{g,\nu}$ on a regular graph.
Indeed, suppose that the diagonal elements of $\lap$ are
constant: that is, there exists $d_0$ such that $d_x = d_0$ for all $x$.
Then the $|\varphi|^4$ Hamiltonian can be written as
\begin{equation}
H_{g,\nu}(\varphi)
  =
\sum_{x\in\vertices}
\left(
  \frac{1}{4} g |\varphi_x|^4
    +
  \frac{1}{2} (\nu + d_0) |\varphi_x|^2
\right)
  -
\frac{1}{2} \varphi \cdot \jay \varphi
\end{equation}
Thus, $d\mu_{g,\nu} \Rightarrow d\mu_J$
if we take the limit $g\to\infty$ with $\nu = -(d_0 + g / 2)$.

The case $n = 1$ is the celebrated \emph{Ising model}. When $n = 2, 3$,
we get the \emph{XY model} and the \emph{classical Heisenberg model}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Phase transitions in infinite volume}

The presence of a phase transition in a physical system is signalled by an abrupt
(i.e.\ non-analytic) change in an observable quantity as a parameter is varied. In fact,
a $p$-th order phase transition is usually said to occur when the free energy
has a discontinuous $p$-th derivative (but continuous derivatives of order less than $p$).
However, the spin systems we have defined above all have smooth free energy (since the
Hamiltonians are smooth functions). Ultimately, the reason we cannot detect phase
transitions in these systems is that we have defined them on finite graphs. Thus, we
are forced to face the problem of defining spin systems on an infinite volume $\graph$.
Here we briefly outline the solution to this problem according to the theory of Dobrushin,
Lanford, and Ruelle.

We wish to define a measure corresponding to an ``infinite-volume Hamiltonian''
\begin{equation}
H^\Phi(\varphi) = \sum \Phi_A(\varphi),
  \quad
\varphi \in \Omega
\end{equation}
where the sum is over all finite subsets $A \subset \vertices$ and the $\Phi_A$ depend only on the
values of $\varphi$ in $A$. Although $H^\Phi$ is typically a fictional object, the $\Phi_A$
are well-defined. Thus, given a finite subset $\Lambda\subset\vertices$ and a choice of
\emph{boundary condition} $\xi\in\Omega$,
we can define the \emph{finite-volume} Hamiltonian $H^\xi_\Lambda : \Omega \to \R$ on $\Lambda$ by
\begin{equation}
H^\xi_\Lambda(\varphi) = \sum \Phi_A(\varphi_{\Lambda} \xi_{\Lambda^c}),
\end{equation}
where the sum is over all finite sets $A$ intersecting $\Lambda$, $\varphi_{\Lambda} \in S^{\Lambda}$
is the restriction of $\varphi$ to $\Lambda$, and $\varphi_{\Lambda} \xi_{\Lambda^c} \in \Omega$
is the concatenation of $\varphi_{\Lambda}$ and $\xi_{\Lambda^c}$.
We then say that $\mu$ is a \emph{Gibbs measure} if for $\mu$-almost
every choice of boundary condition $\xi$, the conditional distribution
$\mu(d\varphi \mid \varphi_{\Lambda^c} = \xi_{\Lambda^c})$ is given by the finite-volume Gibbs
measure
\begin{equation}
\mu^\Phi_\Lambda(\varphi \mid \xi)
  :=
\frac{1}{Z^\Phi_\Lambda(\xi)}
e^{-H^\Phi_\Lambda(\varphi_\Lambda \xi_{\Lambda^c})}
d\lambda^\Lambda(\varphi),
\end{equation}
where $d\lambda^\Lambda(\varphi) = \prod_{x\in\Lambda} d\lambda^0(\varphi_x)$.
We denote the collection of all such Gibbs measures by $\gibbs(\Phi)$.

\subsubsection{The infinite-volume limit}

It can be checked that any finite-volume Gibbs measure (seen as a measure on $\Omega$) is a
Gibbs measure in the above sense. More general Gibbs measures can be constructed by an explicit
procedure known as the infinite-volume limit: generally speaking, one begins with a sequence
$\Lambda_N \subset \vertices$ of finite subsets with $\Lambda_N\uparrow\vertices$. Letting
$H_N$ be a Hamiltonian that only depends on the spins in $\Lambda_N$, one proceeds by taking
the weak limit of the Gibbs measures $\mu_N$ corresponding to $H_N$. If the sequence
$(\Lambda_N, H_N)$ is appropriately chosen, then the weak limit will exist and be an element
of $\gibbs(\Phi)$.

\subsubsection{Periodic boundary conditions}

When $\graph = \Zd$ and the $\Phi$ are translation-invariant\footnote{That is, $\Phi_A = \Phi_{A+i}$
for any $i\in\Zd$.}, there is a particularly convenient approach to taking the infinite-volume
limit. Suppose that $\Phi$ has finite range\footnote{This is a mere convenience. This construction
extends to potentials with infinite range.}, i.e.\ $\Phi_A = 0$ whenever $|A| > R$ for some $R$.
We let $\Lambda_N = \Zd/L^N\Zd$ be the discrete torus of side $L^N$ (for $L > 1$) and identify
the $\Lambda_N$ with an increasing sequence of subsets of $\Zd$ (since $\Phi$ is translation-invariant,
it is not important how this identification is made). Then any subset $A \subset \Zd$ of side
$|A| \le R$ can be identified with a subset of $\Lambda_N$ for $N$ sufficiently large. For such
$N$, we let
\begin{equation}
H_N(\varphi) = \sum \Phi_A(\varphi),
\end{equation}
where the sum is over subsets of $\Lambda_N$ of size at most $R$. \todo{I don't think this is right.}
It is shown in \cite[Example 4.20]{Georgii11} that, if $\mu_N$
converges weakly to a measure $\mu$, then $\mu \in \gibbs(\Phi)$. The measure $\mu$ is said to have
\emph{periodic} boundary conditions.

\begin{example}
Let $\graph = \Zd$ and let $\Lambda_N = \Zd/L^N\Zd$ for $L > 1$.
Let $G_{N,x}$ denote the two-point function of a spin system on $\Lambda_N$.
We define the two-point function on $\Zd$ by the limit
\begin{equation}
G_x = \lim_{N\to\infty} G_{x,N}.
\end{equation}
By the previous example, if the limit $\mu$ of the $\mu_N$ exists, then
\begin{equation}
G_x = \frac{1}{n} \int d\mu(\varphi) \varphi_0 \cdot \varphi_x.
\end{equation}
\end{example}

\subsubsection{Phase transitions}

\todo{To do}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Random polymer models}

In the following, $\interval_T$ will denote one of the following choices (for all $T \ge 0$):
\begin{equation}
\interval_T
  =
\begin{cases}
\{ 0, \ldots, \lfloor T \rfloor \} \\
[0, T]
\end{cases}.
\end{equation}
These two choices will be referred to, respectively, as the \emph{discrete-} and
\emph{continuous-time} cases.
\todo{Unless the graph is transitive, the following depends on $0$.}
Fix a designated vertex $0 \in \vertices$ and
let $\Wcal_T$ denote the set of
right-continuous paths $\omega : \interval_T \to \vertices$ with $\omega(0) = 0$.
We will refer to elements of $\Wcal := \bigcup_{T \geq 0} \Wcal_T$ as \emph{walks}.
A model of walks is determined by a choice of finite measure $d\mu_T$ on
$\Wcal_T$ for each $T$.
In the discrete-time case, we will assume that
$\mu_T = \mu_{\lfloor T \rfloor}$ for all $T$ (both are measures on
$\Wcal_{\lfloor T \rfloor}$ in this case).

Given a model of walks $d\mu_T$, we define the normalizing constant
\begin{equation}
c_T = \int_{\Wcal_T} d\mu_T.
\end{equation}
Given a parameter $\nu \in \R$ (called the \emph{killing rate}),
there is a natural measure $\mu$ on $\Wcal$ defined by
\begin{equation}
\int f \; d\mu
  =
\int_0^\infty dT \; e^{-\nu T} \int_{\Wcal_T} d\mu_T f
\end{equation}
for any continuous function $f$ on $\Wcal$ with compact support.
\todo{When is $\mu$ well-defined?}
The corresponding normalizing constant is denoted
\begin{equation}
\chi(\nu) = \int_0^\infty dT \; e^{-\nu T} c_T
\end{equation}
and is known as the \emph{susceptibility}. In the discrete-time case,
we let $z = e^{-\nu}$ and write
\begin{equation}
\mu(f)
  =
\frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty z^n \int_{\Wcal_n} d\mu_n(\omega) \; f(\omega).
\end{equation}
with
\begin{equation}
\chi(\nu) = \frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty c_n z^n.
\end{equation}

For $a, b \in \vertices$, let $\Wcal_T(a, b) \subset \Wcal_T$ denote the collection of walks
$\omega \in \Wcal_T$ with $\omega_0 = a$ and $\omega_T = x$.
The conditional measure $\mu^{(ab)}_T = \mu_T(\cdot \mid \Wcal_T(a, b))$
is given by
\begin{equation}
\mu^{(ab)}_T(d\omega) = \frac{\mu_T(d\omega) \1_{\Wcal_T(a, b)}(\omega)}{c_T(a, b)},
\end{equation}
where
\begin{equation}
c_T(a, b) = \mu_T(\Wcal_T(a, b)).
\end{equation}
Let $\Wcal(a, b) = \bigcup_{T \ge 0} \Wcal_T(a, b)$. Then
\begin{equation}
\mu^{(ab)}(d\omega) = \frac{\mu(d\omega) \1_{\Wcal(a, b)}(\omega)}{G_x},
\end{equation}
where the \emph{two-point function}
\begin{equation}
G_{ab} = \mu(\Wcal(a, b)) = \int_0^\infty dT \; e^{-\nu T} c_T(a, b).
\end{equation}
In discrete time, we have
\begin{align}
G_{ab} = \frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty c_n(a, b) z^n.
\end{align}

Many models of walks are determined by letting $\mu_T$ be the canonical Gibbs measure
associated to a Hamiltonian $H_T : \Wcal_T \to \R$ and a base measure $d\lambda_T$ on $\Wcal_T$.
That is,
\begin{equation}
d\mu_T(\omega) = \frac{1}{Z_T} e^{-H_T(\omega)} d\lambda_T(\omega).
\end{equation}
In this caes, $\mu$ is the corresponding grand canonical ensemble (with killing rate playing
the role of fugacity) and $c_T$ and $\chi$
are the canonical and grand canonical partition functions, respectively.

% In what follows, we let $Q$ be a $\vertices \times \vertices$ matrix with $Q_{xy} = 0$
% whenever $\{ x, y \} \notin \edges$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Random walk}

\todo{For now specialize to unkilled walks. We only really need the transient case, for which
there is no problem with the corresponding Gaussian measure (in what sense? see Friedli-Velenik).
Put some of this in an external storage file.}

Let $Q$ be the $\vertices\cup\{\partial\}\times\vertices\cup\{\partial\}$ matrix with
\begin{equation}
Q_{xy}
  =
\begin{cases}
J_{xy},       & x \ne y, \partial, y \in \vertices \\
m^2,          & x \ne \partial, y = \partial \\
-(m^2 + d_x), & x = y \in \vertices \\
0,            & x = \partial
\end{cases}.
\end{equation}
Let $X_t$ be the Markov process with generator $Q$.
That is, $X$ takes jumps from $x$ at rate
\begin{equation}
-Q_{xx}
  =
\begin{cases}
m^2 + d_x,    & x = y \in \vertices \\
0,            & x = y = \partial
\end{cases}
\end{equation}
and jumps from $x$ to $y$ with probability (with $0/0 = 1$)
\begin{equation}
\hat P_{xy}
  =
-Q_{xy} / Q_{xx}
  =
\begin{cases}
z_x P_{xy},             & x \ne \partial, y \in \vertices \\
\frac{m^2}{m^2 + d_x},  & x \ne \partial, y = \partial \\
\1_{y=\partial},        & x = \partial
\end{cases}.
\end{equation}
We call $X_t$ the \emph{continuous-time random walk} on $\graph$.
In continuous time, we let
\begin{equation}
\lambda_T(d\omega) = P_0(d\omega \mid \tau_\partial = T),
\end{equation}
where $\tau_\partial = \inf(t : X_T = \partial)$.

The \emph{discrete-time random walk} on $\graph$ is the Markov chain
$\hat X_n$ defined by
\begin{equation}
\hat X_n = X_{\tau_n},
\end{equation}
where $\tau_n$ is the $n$-th jump time of $X$. The transition matrix of
$\hat X$ is given by $\hat P$.

For $v \in \vertices$, let $P_v$ and $E_v$ denote the probability and expectation
with respect to either the process $X$ or $\hat X$ conditioned so that $X_0 = v$.
We have
\begin{equation}
c_T(x, y) = P_x (X_T = y) = (e^{T Q})_{xy}.
\end{equation}
Thus,
\begin{equation}
\int_0^\infty dT \; e^{-\nu T} c_T(x, y)
  =
\sum_{n=0}^\infty \frac{1}{n!} Q^n_{xy} \int_0^\infty dT \; e^{-\nu T} T^n
  =
\sum_{n=0}^\infty Q^n_{xy} \nu^{-(n+1)}
\end{equation}

\begin{example}
We have
\begin{equation}
\int_0^\infty dT \; \1_{X_T=y}
  =
\sum_{n=0}^\infty \int_{\tau_n}^{\tau_{n+1}} dT \; \1_{\hat X_n=y}
  =
\sum_{n=0}^\infty (\tau_{n+1} - \tau_n) \1_{\hat X_n=y}.
\end{equation}
If $\hat X_n = y$, then $\tau_{n+1} - \tau_n$ is a Poisson random variable with rate $d_y$,
so taking the expectation of the above yields
\begin{equation}
G_{xy} = d_y \sum_{n=0}^\infty c_n(x, y).
\end{equation}
\todo{Show that this is}
\begin{equation}
C(x, y) = (m^2 + d_x)^{-1} E_x \left(\sum_{n=0}^\infty \1_{X_n=y} \right)
\end{equation}
so the sum converges if and only if the expected number of visits to $y$
made a walk with transition $\hat P$ started at $x$ is finite. Thus, $C$ is
well-defined if and only if the walk is transient.
In particular, it is well-defined for $m^2 > 0$. When $\graph = \Zd$, it is
well-defined for $m^2 = 0$ if and only if $d > 2$.
\end{example}

\begin{example}
\todo{On transitive graphs, SRW is a sum of iid random variables. This gives CLT and
invariance principle}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Self-avoiding walk}

A \emph{self-avoiding walk} of length $n$ on $\graph$ is a discrete walk $\omega\in\Wcal_n$
that has no self intersections, i.e.\ $\omega_x = \omega_y$ if and only if $x = y$.
We equip the collection of all self-avoiding walks of length $n$ with the uniform measure $\mu_n$.

These measures do not form a consistent family due to the possibility of ``traps''.
That is, the equality
\begin{equation}
\mu_{|\omega|}(\omega) = \sum_{\tilde\omega \supset \omega} \mu_{|\tilde\omega|}(\tilde\omega)
\end{equation}
does not hold for all $\omega\in\Wcal$ (the sum here is over all self-avoiding walks extending
$\omega$).
% \begin{wrapfigure}{R}{0.4\textwidth}
% \vspace{-0.5cm}
% \begin{center}
%   \includegraphics[width=0.3\textwidth]{figures/trapped}
%   \caption{A trapped self-avoiding walk}
%   \label{fig:trap}
% \end{center}
% \vspace{-0.5cm}
% \end{wrapfigure}
For instance, consider the self-avoiding walk $\omega\in\Wcal_7$ on $\Zd$ in
Figure~\ref{fig:trap}.
This walk has positive probability under $\mu_7$ but,
since there are no self-avoiding walks extending $\omega$, the sum on the right-hand side
above is $0$.

As a result, the methods of stochastic processes cannot be directly used to study the self-avoiding
walk. The existence of traps also contributes to the combinatorial difficulty of studying
self-avoiding walk; for instance, it is not clear how to express $c_{n+1}$ (the number of
$(n+1)$-step self-avoiding walks) in terms of $c_n$.

% \begin{figure}[!htb]
% \centering
% \caption{A trapped self-avoiding walk}
% \includegraphics{figures/trapped}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Weakly self-avoiding walk with self-attraction}

Define the \emph{local time} up to time $T$ of $\omega \in \Wcal$ at $x \in \Zd$ by
\begin{equation}
\lt^x_T(\omega) = \int_0^T \1_{\omega(S)=x} \; dS.
\end{equation}
In the discrete-time case, $\lt^x_n$ is the number of times $\omega$ visits $x$
and is bounded by $n$. In the continuous-time case, $\lt^x_T$ is almost surely
finite for the continuous-time simple random walk.

We define the \emph{intersection local time}
\begin{equation}
\label{e:ITdef}
I_T(\omega) = \sum_{x\in\vertices} (\lt^x_T)^2
  =
\int_0^T \!\! \int_0^T \1_{\omega(S_1)=\omega(S_2)} \; dS_1 dS_2
\end{equation}
and the \emph{contact self-attraction}
\begin{equation}
\label{e:CTdef}
C_T(\omega) =
  \sum_{x \in \vertices} \sum_{y \sim x} \lt_T^x(\omega) \lt_T^y(\omega)
  = \int_0^T ds \int_0^T dt \; \1_{\omega_{s} \sim \omega_{t}}
\end{equation}
up to time $T$.
% Recall that we have set the inverse temperature equal to $1$.
Given a parameter $\gcc > 0$,
and $\gamma \in \R$, we define
\begin{equation}
\label{e:Udef-neg}
U_{\gcc,\gamma}(f)
=
\gcc \sum_{x\in\vertices} f_x^2
- \frac{\gamma}{2d}
\sum_{x\in\vertices} \sum_{y \sim x} f_x f_y
\end{equation}
for $f : \vertices \to \R$.
The \emph{weakly self-avoiding walk with self-attraction} (WSAW-SA) is defined via
the Hamiltonian
\begin{equation}
H_T(\omega) = U_{\gcc,\gamma}(L_T(\omega)).
\end{equation}
We denote the canonical partition function by
\begin{equation}
c_T = c_T(\gcc, \gamma) = E_0 \left( e^{-\gcc I(T) + \gamma C(T)} \right),
\end{equation}
where $0 \in \vertices$ is fixed, and the susceptibility by
\begin{equation}
\chi(\gcc, \gamma, \nu) = \int_0^\infty c_T e^{-\nu T} \; dT.
\end{equation}

In the case $\gamma = 0$, the discrete-time version of this model is known as
the \emph{Domb-Joyce model}. In continuous-time, it is the \emph{continuous-time
weakly self-avoiding walk} (WSAW).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Critical behaviour and universality}

\todo{This description of critical behaviour is too narrow and does not apply to walks}

In this section, we let $\graph = \Zd$.

Let $\gibbs_\beta$ denote the set of all Gibbs measures for some potential at inverse temperature
$\beta$. A \emph{phase transition} is said to occur at inverse temperature $\beta$ if
$|\gibbs_\beta| > 1$. Many systems possess a unique \emph{critical point} $\beta_c$, such that
\begin{equation}
|\gibbs_\beta|
\begin{cases}
=1,  & \beta < \beta_c \\
> 1, & \beta > \beta_c
\end{cases}
\end{equation}
and it is usually expected that $|\gibbs_{\beta_c}| = 1$.
Moreover, such systems tend to exhibit \emph{critical behaviour} when $\beta = \beta_c$:
roughly speaking, this means that a number of observables scale according to a power
law (sometimes with logarithmic corrections) at or near $\beta_c$, whereas these same
observables exhibit exponential decay away from $\beta_c$.

A strong indicator of critical behaviour is the divergence of the
\emph{correlation length}, defined for any model (of walks or spins) with two-point
function $G_x(\mu)$ by
\begin{equation}
\xi(\mu) = \limsup_{k\to\infty} \frac{-k}{\log G_{ke}(\mu)},
\end{equation}
where $e \in \Zd$ is a unit vector.
In other words, the correlation length is the exponential rate of decay of the
two-point function.
A related quantity is the \emph{correlation length of order $p$}, defined by
\begin{equation}
\xi_p(\mu) = \left(\frac{\sum_{x\in\Zd} |x|^p G_x(\mu)}{\chi(\mu)}\right)^{1/p}.
\end{equation}
\todo{Heuristic relation bewteen $\xi$ and $\xi_p$.}

It is expected that
\begin{align}
G_x       &\sim C_1 |x|^{-(d - 2 + \eta)}, \\
\chi(\nu) &\sim C_2 (\nu - \nu_c)^{-\gamma}, \\
\xi       &\sim C_3 (\nu - \nu_c)^{-\nubar}, \\
\xi_p     &\sim C_4 (\nu - \nu_c)^{-\nubar}
\end{align}
possibly with logarithmic corrections.
For walks, it is expected that
\begin{align}
c_T                       &\sim C_5 e^{-\nu_c T} T^{-\gamma}, \\
\langle |X_T|^2 \rangle   &\sim C_6 T^{-\nubar}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The DGFF}

The two-point function is just the massive Green function $(-\Delta + m^2)^{-1}$
which, on $\Zd$, has the well-known Ornstein-Uhlenbeck decay \todo{(show this;
use random walks?)}.
Moreover,
\begin{equation}
\chi
  =
\sum_{x\in\vertices} (-\Delta + m^2)^{-1}_{0x}
  =
\sum_{x\in\vertices} \sum_{n=0}^\infty z^n P^n_{0x}
  =
\sum_{n=0}^\infty z^n
  =
(1 - z)^{-1}.
\end{equation}
Thus, there is a critical point at $m^2 = 0$ ($z = 1$).

\todo{See candidacy report or preliminary version of it.}

\todo{For the Green function, see Theorem 1.5.4 in Lawler--Intersections of Random Walks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Universality}

Critical behaviour should, roughly speaking, only depend on ``tail properties''
(global geometry, range of interaction, and symmetries).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Scaling limits}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Relations between models}

There are a number of close relationships between models of walks and ferromagnetic spin
systems given by a Gibbs measure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The SRW and DGFF}

\todo{See candidacy report or preliminary version of it. See also misc notes}

For the discrete-time simple random walk, the canonical partition function $c_n$,
which is the number of such walks, is just $(2 d)^n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Kac-Siegert representation}

\todo{See notes on this}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Self-avoiding walk representations}

\todo{High-temperature expansion of spin system (can be used later to motivate polymer
expansion) to get loop models, De Gennes $n\downarrow0$ limit, McKane/Parisi-Sourlas
and supersymmetry, Grassmann integration and BIS representation (sufficiently general
for WSAW-SA)}