% Parts are the largest structural units, but are optional.
%\part{Thesis}

% Chapters are the next main unit.
\chapter{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical mechanics}

\todo{Restructure this: just introduce Gibbs measure, don't make hard distinction
between canonical and grand canonical. Only mention GCE when discussing walks
later}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Entropy}

Let $(\Omega, \lambda)$ be a measure space. The state of knowledge of a system on
$\Omega$ can be expressed by a probability measure $\mu$ on $\Omega$. Let
$\Mcal_\lambda(\Omega)$ denote the set of probability measures on $\Omega$ absolutely
continuous with respect to $\lambda$. For
$\mu \in \Mcal_\lambda(\Omega)$, we denote the Radon-Nikodym derivative of
$\mu$ with respect to $\lambda$ by $d\mu/d\lambda$ and define the
\emph{entropy} of $\mu$ with respect to $\lambda$ by
\begin{equation}
h(\mu) = h_\lambda(\mu) = -\int_\Omega \log\frac{d\mu}{d\lambda} \; d\mu.
\end{equation}
In many cases, specific information about the system under consideration is available,
so that we may restrict our attention to a subspace $M \subset \Mcal_\lambda(\Omega)$.
% for example in the form of statements such as
% \begin{equation}
% \int f \; d\mu \in S_f
% \end{equation}
% with $f$ running over some collection of $\mu$-integrable functions
% that represent \emph{observable} quantities of the system, and $S_f$ a Borel
% subsets of $\R$ for each $f$.
The \emph{principle of maximum entropy} \cite{Jaynes57} asserts that, in this
case, the measure best expressing the state of knowledge of the system is given by
\begin{equation}
\hat\mu = \argmax(h_\lambda(\mu) : \mu \in M),
\end{equation}
assuming such a measure exists and is unique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Microcanonical ensemble}

Consider an \emph{isolated} physical system on $\Omega$, that is, one that cannot
exchange energy with its surroundings. Such a system can be determined by a choice
of function $H : \Omega \to \R$, called the \emph{Hamiltonian}. The value $H(\omega)$
represents the total energy of the system in state
$\omega\in\Omega$.

\begin{example}
Let $\Omega = U^n \times \R^{3n}$, where $U \subset \R^3$, and denote a generic
element of $\Omega$ by $(q, p)$, where $q \in U^n$ and $p \in \R^{3n}$. Then
$\Omega$ is the state space of a system of $n$ point particles
$i = 1, \ldots, n$ with positions $q_i \in U$ and momenta $p_i \in \R^3$. Given
a $C^1$ Hamiltonian $H : \Omega \to \R$, the dynamics of such a system is determined
by \emph{Hamilton's equations}
\begin{align}
\dd{q}{t}   &= \nabla_q H(q(t), p(t)) \\
-\dd{p}{t}  &= \nabla_p H(q(t), p(t)).
\end{align}
An immediate consequence of these equations and the chain rule is the \emph{principle of conservation of energy}:
\begin{equation}
\dd{H}{t}(q(t), p(t)) = 0.
\end{equation}
Thus, a Hamiltonian system with initial configuration $(q(0), p(0))$ of energy
$E = H(q(0), p(0))$ will evolve on the constant energy shell $S_E = H^{-1}(E)$.

% Let us assume that $H$ has compact level sets so that the solutions to Hamilton's
% equations can be extended globally in time. In general, especially when $n$ is large,
% a Hamiltonian system is likely to be too complicated to understand in a precise way.
% The basic idea of statistical mechanics is to leverage this complexity by assuming
% (or in some cases proving) that, as $t\to\infty$, such a system will settle into a
% state of equilibrium in which all possible states are ``equiprobable''.
\end{example}

% Although one can make sense of the notion of an equiprobable probability distribution
% on $S_E$ in the above example, it is simpler (and more relevant to this thesis) to
% consider the case when $S_E$ is finite.
If $S_E = H^{-1}(E)$ is finite\footnote{We can view such a system as an approximation to a traditional continuous system with $S_E$ uncountable.}, then one can easily determine that the maximum entropy measure on $S_E$ is simply the uniform measure. For a system with finite state space $\Omega$ and Hamiltonian $H$, we define the \emph{microcanonical distribution} with energy $E$ to be the uniform measure on $S_E$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Canonical ensemble}

In practice, most systems of interest are not truly isolated: they may exchange
energy with their environments. Everyday experience, however, suggests that
a physical system that is left undisturbed for a sufficiently long time will achieve \emph{thermal equilibrium}, in which the system's temperature is constant and equal to that of its surroundings. We define the \emph{canonical ensemble} for a system with state space $\Omega$ and Hamiltonian $H$ on
$\Omega$ to be the maximum entropy distribution subject to the fixed average energy constraint
\begin{equation}
\int H \; d\mu = E.
\end{equation}
It can be shown by the method of Lagrange multipliers that the canonical ensemble is given by the \emph{Gibbs measure}
\begin{equation}
d\mu_\beta = \frac{1}{Z_\beta} e^{-\beta H} d\lambda,
\end{equation}
where
\begin{equation}
Z_\beta = \int e^{-\beta H} \; d\lambda
\end{equation}
is the normalizing constant, known as the \emph{canonical partition function}.
The quantity $\beta = \beta(E)$, which arises as a Lagrange multiplier, is known as the \emph{inverse temperature}.

The \emph{free energy} of this system is defined by
\begin{equation}
F_\beta = -\frac{1}{\beta} \log Z_\beta.
\end{equation}
This definition may seem obscure at first, but is elucidated by a computation of the entropy $h_\lambda(\mu_\beta)$ with $\beta = \beta(E)$, which implies that
\begin{equation}
F_\beta = E - \frac{1}{\beta} h_\lambda(\mu_\beta).
\end{equation}
This is the famous thermodynamic relation between free energy, internal energy $E$, temperature $1/\beta$, and entropy.


In the context of spin systems, there is a natural mathematical reason for studying measures of the above form. This is the Hammersley-Clifford theorem, which states that any Markov random field (a spatial generalization of a Markov chain) on a graph has a representation as a Gibbs measure whose Hamiltonian is a sum of ``local'' interactions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Graphs}

Most systems of interest do not have a finite (or even countable) state space. Nevertheless, finite systems serve as natural approximations of real systems. For instance, a natural way to approximate spatially-extended systems is by defining models on graphs.

An \emph{undirected graph} is a pair $\graph = (\vertices, \edges)$, where $\vertices$
is a set of \emph{vertices} and $\edges$ is a set of
\emph{edges} $\{ x, y \}$ with $x, y \in \vertices$; we will write $x \sim y$ if
$\{ x, y \} \in \edges$.
For simplicity, we will assume that $\vertices$ is countable, that there are no
\emph{self-loops} $\{ x \} \in \edges$, and that $\graph$ is \emph{vertex-transitive};
the last condition means that for all pairs of distinct vertices $x, y \in \vertices$,
there exists a mapping $f : \vertices \to \vertices$ such that $a \sim b$ if and only
if $f(a) \sim f(b)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Functions on graphs}

Let $\varphi \in (\R^n)^\vertices$. We denote the components of $\varphi$ by
$\varphi^i_x \in \R$ for $x \in \vertices$ and $i = 1, \ldots, n$. The Euclidean inner product on $(\R^n)^\vertices$ is defined by
\begin{equation}
\varphi\cdot\tilde\varphi
  =
\sum_{x\in\vertices} \varphi_x \cdot \tilde\varphi_y
  =
\sum_{i=1}^n \sum_{x\in\vertices} \varphi^i_x \tilde\varphi^i_x
\end{equation}
and the Euclidean norm $|\cdot|$ is defined by
\begin{equation}
|\varphi|^2 = \varphi \cdot \varphi.
\end{equation}
A $\vertices\times\vertices$ matrix $M$ acts on $\varphi$ via
\begin{equation}
(M \varphi)_x = \sum_{y\in\vertices} M_{xy} \varphi_y.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The graph Laplacian}

Let $\jay$ be a $\vertices\times\vertices$ matrix with $\jay_{xy} \ge 0$ for all $x, y$ with equality if and only if $\{ x, y \} \notin \edges$. The pair
$(\graph, \jay)$, which we will usually abbreivate as $\graph$ with $\jay$ implicit, is an example of a \emph{weighted} graph.

Define the diagonal $\vertices\times\vertices$ matrix $\diag$ by
\begin{equation}
\diag_{xx} = d_x = \sum_{y \sim x} J_{xy}.
\end{equation}
We will say that $\graph$ is $d_0$-regular if $d_x = d_0$ for all $x$.

The \emph{(massless) graph Laplacian} on $\graph$ is defined by
\begin{equation}
-\lap = \diag - \jay.
\end{equation}
We also define the \emph{massive Laplacian} with squared \emph{mass} $m^2 > 0$
by
\begin{equation}
-\lap + m^2.
\end{equation}
Note that
\begin{equation}
\varphi \cdot (-\lap \varphi)
  =
\frac{1}{2} \sum_{x,y\in\vertices} J_{xy} |\varphi_x - \varphi_y|^2
  \ge
0,
\end{equation}
so $-\lap$ is positive-semidefinite.

\begin{example}
An important case is when $\jay$ has $\{0, 1 \}$-valued entries. In this case, $d_x$ is the \emph{degree} of $x$ in $\graph$ and we denote $\lap$ by
$\Delta$, which has entries given by
\begin{equation}
-\Delta_{xy} = d_x \1_{x=y} - \1_{x \sim y}.
\end{equation}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Green function}

If $m^2 > 0$, then $-\lap + m^2$ is positive-definite, hence invertible with inverse
\begin{equation}
(-\lap + m^2)^{-1} = (m^2 + D)^{-1} \sum_{n=0}^\infty Z^n P^n,
\end{equation}
where
\begin{align}
Z = (m^2 + D)^{-1} D,
  \quad
P = D^{-1} J.
\end{align}
Let $z_x$ denote the diagonal elements of $Z$. The \emph{Green function} for
$-\lap + m^2$ is the kernel of $(-\lap + m^2)^{-1}$, given by
\begin{equation}
C(x, y)
  =
(m^2 + d_x)^{-1} \sum_{n=0}^\infty z_x^n P^n_{xy}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Spin systems}

\subsection{The Ising model}

The Ising model is defined by the Gibbs measure on $\Omega = \{ \pm 1 \}^\vertices$
with Hamiltonian
\begin{equation}
H(\sigma) = -\frac{1}{2} \sum_{x \sim y} \sigma_x \sigma_y + h \sum_{x\in\vertices} \sigma_x,
\end{equation}
where $h \in \R$.
The magnetization is defined by
\begin{equation}
\langle \sigma_0 \rangle = \frac{1}{Z_\beta} \sum_{\sigma\in\Omega} \sigma_0 e^{-\beta H(\sigma)}.
\end{equation}
The \emph{magnetic susceptibility} is the derivative
\begin{equation}
\chi
	=
-\frac{1}{\beta} \dd{}{h} \langle \sigma_0 \rangle.
\end{equation}
A computation shows that
\begin{equation}
\chi = \sum_{x\in\vertices} G_x,
\end{equation}
where
\begin{equation}
G_x = \Big(\langle \sigma_0 \sigma_x \rangle - \langle \sigma_0 \rangle \langle \sigma_x \rangle\Big)
\end{equation}
is the \emph{two-point function}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ferromagnetic spin systems}

An $n$-component \emph{field} or \emph{spin configuration} on $\vertices$
with spins in $S \subset \R^n$ is an element of $\Omega = S^\vertices$.
% A \emph{spin system} is a probability measure $d\mu$ on $\Omega$.
Suppose that $S$ is endowed with a measure $d\lambda^0$.
Given a (measurable) Hamiltonian function $H : \Omega \to \R$, we wish to define the measure
\begin{equation}
d\mu_\beta(\varphi)
  =
\frac{1}{Z_\beta} e^{-\beta H(\varphi)} d\lambda(\varphi)
\end{equation}
on $\Omega$, where
\begin{equation}
d\lambda(\varphi) = \prod_{x\in\vertices} d\lambda^0(\varphi_x).
\end{equation}
However, there are some problems with this definition when $\vertices$ is infinite.
For one, $d\lambda$ may not be well-defined (for instance if
$S = \R$ and $d\lambda^0$ is Lebesgue measure). Another issue is that it may be
difficult to define a reasonable choice of $H$ on the infinite product space
$\Omega$.

For this reason, we temporarily restrict our attention to finite graphs:
\begin{equation}
|\vertices| < \infty.
\end{equation}
Then the Gibbs measure $\mu = \mu_\beta$ is well-defined and we will denote the
expectation with respect to this measure by $\langle \cdot \rangle_\mu$.

Following our discussion of the Ising model, we define the two-point function
and susceptibility by
\begin{equation}
G_x(\mu_\beta)
  =
\frac{1}{n}
\big(\langle \varphi_0 \cdot \varphi_x \rangle_\mu
  -
\langle \varphi_0 \rangle_\mu \cdot \langle \varphi_x \rangle_\mu\big)
\end{equation}
and
\begin{equation}
\chi = \sum_x G_x.
\end{equation}

We will mainly be concerned with \emph{ferromagnetic} spin systems, for which the Hamiltonian has the form
\begin{equation}
H(\varphi) = \sum_{x, y \in \vertices} \varphi_x \cdot J_{xy} \varphi_y,
\end{equation}
where (recall) $J_{xy} \ge 0$. Thus, $H(\varphi)$ is smaller (and $d\mu_\beta(\varphi)$ larger) when the spins align (when $\varphi_x = \varphi_y$ for $x \sim y$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{The \texorpdfstring{$O(n)$}{O(n)} spin model}

\begin{example}[The $O(n)$ model]
Let $S = S^{n-1} \subset \R^n$ be the unit $(n-1)$-sphere equipped with the surface measure $d\lambda^0$ (in particular, $S^0 = \{ \pm 1 \}$, which is equipped with the counting measure). The $O(n)$ model is the
ferromagnetic spin sysmste with Hamiltonian
\begin{equation}
H_J(\sigma) = -\frac{1}{2} \sigma \cdot J \sigma,
\end{equation}
which is clearly ferromagnetic. The case $n = 1$ is the celebrated \emph{Ising model}. When $n = 2, 3$, we get the \emph{XY model} and the \emph{classical Heisenberg model}.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{The \texorpdfstring{$|\varphi|^4$}{phi4} spin model}

\begin{example}[The $|\varphi|^4$ model]
Let $S = \R^n$. The $|\varphi|^4$ spin model on $\graph$ is the ferromagnetic spin system
defined by the quartic Hamiltonian
\begin{equation}
H_{g,\nu}(\varphi)
  =
\sum_{x\in\vertices}
\left(
  \frac{1}{4} g |\varphi_x|^4
    +
  \frac{1}{2} \nu |\varphi_x|^2
    +
  \frac{1}{2} \varphi_x \cdot (-\lap \varphi)_x
\right),
\end{equation}
where $g > 0$ and $\nu\in\R$. This is a natural generalization of the $O(n)$ model
in which spins are merely concentrated near a sphere.
Indeed, writing
\begin{equation}
H_{g,\nu}(\varphi)
  =
\sum_{x\in\vertices}
\left(
  \frac{1}{4} g |\varphi_x|^4
    +
  \frac{1}{2} (\nu + d_0) |\varphi_x|^2
\right)
  -
\frac{1}{2} \varphi \cdot \jay \varphi
\end{equation}
we see that $d\mu_{g,\nu} \Rightarrow d\mu_J$ if we take the limit $g\to\infty$
with $\nu = -(d_0 + g / 2)$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Gaussian measures and the free field}

\begin{example}[Gaussian free field]
Let $S = \R^n$ and let $C$ be a positive-definite symmetric $\vertices\times\vertices$ matrix. The $n$-component \emph{Gaussian measure} $d\mu_C$ on
$\Omega$ with mean $0$ and \emph{covariance} $C$ is defined by the Hamiltonian
\begin{equation}
H_C(\varphi) = \frac{1}{2} \varphi \cdot C^{-1} \varphi.
\end{equation}
This is essentially the simplest choice of non-constant
Hamiltonian\footnote{Note that $e^{-\beta H}$ is not integrable if $H$ is linear. Also, by completing the square, there is no additional generality gained by adding a linear term to $H_C$.}.
The partition function can be computed explicitly, giving\footnote{Here, we have employed our convention of setting $\beta = 1$ when the Hamiltonian depends on a parameter ($C$ in this case).}
\begin{equation}
d\mu_C(\varphi)
  =
\frac{1}{\sqrt{\det(2\pi C)}} e^{-\frac{1}{2} \varphi\cdot C^{-1}\varphi} d\varphi.
\end{equation}
By \emph{Wick's theorem}, the magnetization is $0$ and the two-point function is the covariance:
\begin{equation}
\int \varphi_a \cdot \varphi_b \; d\mu(\varphi) = C_{ab}.
\end{equation}

An important case is the \emph{massive Gaussian free field} on $\graph$,
which is the $g = 0$ case of the $|\varphi|^4$ model (with $\nu$ necessarily positive).
Thus, the the covariance is equal to the massive Green function $C = (-\lap + \nu)^{-1}$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The infinite-volume limit}

The presence of a phase transition in a physical system is signalled by an abrupt (i.e.\ non-analytic) change in an observable quantity as a parameter is varied. In fact, a $p$-th order phase transition is usually said to occur when the free energy has a discontinuous derivative of order $p$ with respect to an external field (but continuous derivatives of order less than $p$). However, the spin systems we have defined above all have smooth free energy (since the Hamiltonians are smooth functions). Ultimately, the reason we cannot detect phase transitions in these systems is that we have defined them on finite graphs. Thus, we are forced to face the problem of defining spin systems on an infinite graph.

A natural approach to defining such systems is via a procedure known as the
\emph{infinite-volume limit}, which we describe here. For any
finite subgraph $\Lambda \subset \graph$, let $H_\Lambda$ be the Hamiltonian
of one of the above spin systems on $\Lambda$ and let $\mu_\Lambda$ be the
corresponding Gibbs measure (which we can view as a measure on the full state
space $\Omega$).

Let $\Lambda_N \subset \graph$ be a sequence of subgraphs that exhaust
$\graph$, i.e.\ $\Lambda_N \uparrow \graph$. If the limits
\begin{equation}
\lim_{N\to\infty} \int f \; d\mu_{\Lambda_N}
\end{equation}
exist for a sufficiently rich class of functions $f$ (such as all bounded continuous functions), then they define a measure $\mu$ on $\Omega$ (with $\mu(f)$ the above limit), which we call a \emph{Gibbs state} or
\emph{infinite-volume Gibbs measure} on $\Omega$.

We remark that there is a more general approach to the study of spin systems in infinite volume developed by Dobrushin, Lanford, and Ruelle. We do not detail their approach here, but merely mention that, for the examples above, this approach involves defining a Gibbs measure for the collection
$H = (H_\Lambda)$ of Hamiltonians directly as a measure on $\Omega$ satisfying a system of constraints on its conditionals measures. This is somewhat in the spirit of Kolmogorov's consistency conditions with the importance difference that the resulting collection $\gibbs_\beta(H)$ of Gibbs states at inverse temperature
$\beta$ need not consist of only a single element. This is significant due to the interpretation of distinct elements of $\gibbs(H)$ as corresponding to different phases of the system under consideration.

For many models, including the $O(n)$ and $n$-component $|\varphi|^4$ models on $\Zd$ with $d > 2$, it is known that there exists a critical inverse temperature $\beta_c < \infty$ such that
$|\gibbs_\beta(H)| = 1$ if and only if $\beta \le \beta_c$ (\REF). In this thesis, our main concern is with the behaviour at $\beta_c$ and as
$\beta \uparrow \beta_c$. Thus, we need not concern ourselves with the precise nature of the infinite-volume limit.

\begin{rk}
Our interest will be in translation-invariant systems on $\Zd$, for which a particular approach to the infinite-volume limit will be convenient. Namely, for $L > 1$ we will let $\Lambda_N = \Zd/L^N\Zd$ be the discrete torus, which we view as a subset of $\Zd$. This allows us to preserve
translation-invariance of these models in finite volume. Strictly speaking, $\Lambda_N$ is not a subgraph of $\Zd$; nevertheless, it can be shown that the infinite-volume limit (if it exists) is a Gibbs measure in the usual sense; see \cite[Example 4.20]{Georgii11} for details.
\end{rk}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Critical behaviour}

For this discussion, let us restrict our attention to the graph $\graph = \Zd$
with $J_{xy} \in \{ 0, 1 \}$. Thus, $L = \Delta$.

\begin{example}
Let $\langle \cdot \rangle_{0,\nu}$ denote the expectation with respect to the
Gaussian free field with mass $\nu$. We have
\begin{equation}
-\Delta = 2 d (1 - P),
\end{equation}
where $P = (2 d)^{-1} J$ is the transition matrix for the simple random walk $X$ on $\Zd$.
For $d > 2$, the simple random walk is transient. Thus, letting $E_0$ denote the expectation
of $X$ conditioned so that $X_0 = 0$,
\begin{equation}
(1 - P)^{-1}_{0x} = \sum_{n=0}^\infty P^n_{0x} = E_0 \sum_{n=0}^\infty \1_{X_n=x} < \infty
\end{equation}
and we can define the \emph{massless} Gaussian free field on $\Zd$ to be the Gaussian
measure with covariance $(-\Delta)^{-1} = (2 d)^{-1} (1 - P)^{-1}$.


\end{example}

Many systems exhibit \emph{critical behaviour} at or near a particular parameter value. For concreteness, in the setting of the models discussed above, we denote this parameter value here by $\beta_c$ and suppose that it is defined as the small valuest of $\beta$ for which the susceptibility diverges. For spin systems, this is typically the same value that separates the uniqueness regime in which $|\gibbs_\beta| = 1$ from the non-uniqueness regime in which
$|\gibbs_\beta| > 1$.

The critical point is characterized by long-range correlations. Namely, for
$\beta \ne \beta_c$, the two-point function $G_x$ decays exponentially in $x$. Clearly, this cannot be the case at $\beta_c$ if the susceptibility is to diverge and it is expected that $G_x$ scales according to a power there 
(possibly with logarithmic corrections).

A fundamental quantity in the study of critical behaviour is the
\emph{correlation length} $\xi$. On $\Zd$, this is defined to be one over the exponential rate of decay of the two-point function; that is,
\begin{equation}
\xi(\mu) = \limsup_{k\to\infty} \frac{-k}{\log G_{ke}(\mu)},
\end{equation}
where $e \in \Zd$ is a unit vector, whose choice is irrelevant for models for which the two-point function is invariant under lattice rotations.
A related quantity is the \emph{correlation length of order $p$}, defined by
\begin{equation}
\xi_p(\mu) = \left(\frac{\sum_{x\in\Zd} |x|^p G_x(\mu)}{\chi(\mu)}\right)^{1/p}.
\end{equation}
\todo{Heuristic relation bewteen $\xi$ and $\xi_p$.}

We define the \emph{critical exponents} $\eta$, $\gamma$, and $\bar\nu$ by
\begin{align}
G_x       &\sim C_1 |x|^{-(d - 2 + \eta)}, \\
\chi(\nu) &\sim C_2 (\nu - \nu_c)^{-\gamma}, \\
\xi       &\sim C_3 (\nu - \nu_c)^{-\nubar}, \\
\xi_p     &\sim C_4 (\nu - \nu_c)^{-\nubar}
\end{align}
when these relations hold. For walks, it is expected that
\begin{align}
c_T                       &\sim C_5 e^{-\nu_c T} T^{-\gamma}, \\
\langle |X_T|^2 \rangle   &\sim C_6 T^{-\nubar}.
\end{align}
\todo{Motivate the above with an example somewhere.}

\begin{example}
The two-point function is just the massive Green function
$(-\Delta + m^2)^{-1}$ which, on $\Zd$, has the well-known Ornstein-Uhlenbeck decay \todo{(show this; use random walks?)}. Moreover,
\begin{equation}
\chi
  =
\sum_{x\in\vertices} (-\Delta + m^2)^{-1}_{0x}
  =
\sum_{x\in\vertices} \sum_{n=0}^\infty z^n P^n_{0x}
  =
\sum_{n=0}^\infty z^n
  =
(1 - z)^{-1}.
\end{equation}
Thus, there is a critical point at $m^2 = 0$ ($z = 1$).

\todo{See candidacy report or preliminary version of it. For the Green function, see Theorem 1.5.4 in Lawler--Intersections of Random Walks}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Models of walks}

\subsection{Discrete-time walks}

For simplicity, suppose that $J_{xy} \in \{ 0, 1 \}$ and that $\graph$ is
$d_0$-regular.

Let $[n] = \{ 0, \ldots, n \}$.
An $n$-step \emph{walk} is a function $\omega : [n] \to \vertices$ with
$\omega_i \sim \omega_{i+1}$ for all $i \in [n]$. Let $\dwalks_n$ denote the collection
of $n$-step walks $\omega$ with $\omega_0 = 0$ and let $\dwalks = \bigcup_n \dwalks_n$.
Given $\omega\in\dwalks_n$, we write $|\omega| = n$.
A model of discrete-time walks is defined by a weight function $w : \dwalks \to \R$.

The \emph{generating function} of a sequence $c_n$ is the function defined by
the power series
\begin{equation}
\sum_n c_n z^n.
\end{equation}
Let $\dwalks_n(x)$ denote the collection of walks $\omega\in\dwalks_n$ with $\omega_n = x$
and set $\dwalks(x) = \bigcup_n \dwalks_n(x)$.
The two-point function $G_x$ for walks with weight function $w$ is the generating
function of the sequence
\begin{equation}
c_n(x) = \sum_{\omega\in\dwalks_n(x)} w(\omega).
\end{equation}
That is,
\begin{equation}
G_x = \sum_n z^n c_n(x)
	= \sum_{\omega\in\dwalks(x)} w(\omega) z^{|\omega|}.
\end{equation}
The susceptibility is the generating function for $c_n = \sum_x c_n(x)$:
\begin{equation}
\chi = \sum_n c_n z^n = \sum_x G_x.
\end{equation}

We define a probability measure $\mu_n$ on $\dwalks_n$ by $\mu(\omega) = w(\omega) / c_n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{example}[Simple random walk]
The (discrete-time) simple random walk $X_n$ is the Markov chain on $\graph$ with
transition matrix $P = d_0^{-1} J$. This is a model of walks in the sense above
with $w(\omega) = 1$ for all $\omega$.

There is a close relationship between the
simple random walk and the Gaussian free field. Since $P$ is a stochastic matrix,
the two-point function converges for $|z| < 1$. For $0 < z < 1$, we can write
$z = d_0 / (d_0 + m^2)$ with $m^2 > 0$ to get
\begin{equation}
G_x = \sum_n P^n_{0x} z^n = (2 d + m^2) (-\Delta + m^2)^{-1}_{0x}.
\end{equation}
In other words, the two-point function for simple random walk is the two-point
function for the Gaussian free field.

When $z = 1$, the power series above is the expected number of visits $X$ makes
to $x$ when started at $0$. Thus, $G_x$ converges at $z = 1$ when $X$ is transient.
% Since $\sum_x P^n_{0x} = 1$
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Self-avoiding walk}

\begin{example}[Self-avoiding walk]
A walk $\omega$ is said to be \emph{self-avoiding} if $\omega_i \ne \omega_j$
for all $i \ne j$. Let $\Scal_n \subset \dwalks_n$ denote the set of $n$-step
self-avoiding walks starting at $0$. We define the weights $w(\omega) = \1_{\omega\in\Scal_n}$.
Then $c_n(x)$ is the number of $n$-step self-avoiding walks from $0$ to $x$
and $c_n = |\Scal_n|$.

Note that the sequence $c_n$ is sub-multiplicative: $c_{m+n} \le c_m c_n$.
Thus, Fekete's lemma implies that the existence of the \emph{connective constant}
$c(\graph)$ of $\graph$, defined by
\begin{equation}
c(\graph) = \lim_{n\to\infty} n^{-1} \log c_n.
\end{equation}
Note that, by the trivial bounds $1 \le c_n \le d_0^n$ for $n \ge 1$, $c(\graph) \in [0, d_0]$.
% Roughly speaking, this means that $c_n \approx c(\graph)^n$ for large $n$.
By definition, the susceptibility has radius of convergence $c(\graph)^{-1}$.
Since $c_n \ge 0$ for all $n$, the susceptibility diverges at $c(\graph)^{-1}$.

The measure $\mu_n$ on $\dwalks_n$ is the uniform measure.
These measures do not form a consistent family due to the possibility of ``traps''. That is, the equality
\begin{equation}
\mu_{|\omega|}(\omega) = \sum_{\tilde\omega \supset \omega} \mu_{|\tilde\omega|}(\tilde\omega)
\end{equation}
does not hold for all $\omega\in\dwalks$ (the sum here is over all self-avoiding walks extending $\omega$).
% \begin{wrapfigure}{R}{0.4\textwidth}
% \vspace{-0.5cm}
% \begin{center}
%   \includegraphics[width=0.3\textwidth]{trapped}
%   \caption{A trapped self-avoiding walk}
%   \label{fig:trap}
% \end{center}
% \vspace{-0.5cm}
% \end{wrapfigure}
For instance, consider the self-avoiding walk $\omega\in\dwalks_7$ on $\Zd$ in
Figure~\ref{fig:trap}. This walk has positive probability under $\mu_7$ but,
since there are no self-avoiding walks extending $\omega$, the sum on the 
right-hand side above is $0$.

As a result, the methods of stochastic processes cannot be directly used to
study the self-avoiding walk. The existence of traps also contributes to the
combinatorial difficulty of studying self-avoiding walk; for instance, it is
not clear how to express $c_{n+1}$ (the number of $(n+1)$-step self-avoiding walks)
in terms of $c_n$.

% \begin{figure}[!htb]
% \centering
% \caption{A trapped self-avoiding walk}
% \includegraphics{trapped}
% \end{figure}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gibbs measures on walks}

% We let $\interval_T$ denote either of the following choices for all $T \ge 0$:
% \begin{equation}
% \interval_T
%   =
% \begin{cases}
% \{ 0, \ldots, \lfloor T \rfloor \} \\
% [0, T]
% \end{cases}.
% \end{equation}
For any right-continuous function $\omega : [0, T] \to \vertices$, we define
$\tau_n = \tau_n(\omega)$ inductively by setting $\tau_0 = 0$ and
\begin{equation}
\tau_{n+1} = \inf(t > \tau_n : \omega_t \ne \omega_{\tau_n}).
\end{equation}
We call $\omega$ a \emph{walk} of length $T$ if
$\{ \tau_n : n \in \Z_+ \}$ does not have any cluster points and
$\omega_{\tau_n} \sim \omega_{\tau_{n+1}}$ for all $n$; thus, $\omega$
only jumps between neighbouring vertices.
% A \emph{discrete-time} (respectively,
% \emph{continuous-time}) walk of length $T$ is a walk indexed by
% $\{ 0, \ldots, \lfloor T \rfloor \}$ (respectively, $[0, T]$).
A \emph{discrete-time} walk is one for which $\tau_n = n$ for all $n$.
Let $\cwalks_T$ denote the set of walks of length $T$ with $\omega_0 = 0$
and set $\cwalks := \bigcup_{T \geq 0} \cwalks_T$.

A model of walks is determined by a choice of finite measure $d\mu_T$ on
$\cwalks_T$ for each $T$. Our focus will be on canonical Gibbs measures of the form
\begin{equation}
\int f \; d\mu_T = \frac{1}{c_T} E_0 (f e^{-H_T}),
\end{equation}
where $E_0$ is the expectation for a random walk on $\graph$ (in discrete or continuous time), $H_T : \cwalks_T \to \R$, and we have denoted the canonical partition function by $c_T$.
% \begin{equation}
% d\mu_T(\omega) = \frac{1}{c_T} e^{-H_T(\omega)} \; d\lambda_T(\omega),
% \end{equation}
% with respect to some measure $\lambda_T$ on $\cwalks_T$ (we have denoted the partition function by $c_T$).
% We will assume that models of discrete-time walks satisfy $\mu_T = \mu_{\lfloor T \rfloor}$ for all $T$
% (note that both are measures on $\cwalks_{\lfloor T \rfloor}$ in this case).
Given such a model, the corresponding grand canonical ensemble is given by
\begin{equation}
d\mu(\omega)
  =
\frac{1}{\chi(\nu)}
e^{-\nu |\omega| - H_{|\omega|}(\omega)}
d\lambda_{|\omega|}(\omega)
\end{equation}
and the grand canonical partition function, denoted $\chi(\nu)$, is known as the \emph{susceptibility}. In this setting, the fugacity $\nu$ is usually referred to as the \emph{killing rate}.
% The grand canonical partition function, denoted $\chi$, is known as the \emph{susceptibility}.
Note that
\begin{equation}
\mu(\cdot \mid \cwalks_T) = \mu_T.
\end{equation}

Let $\cwalks_T(x) \subset \cwalks_T$ denote the set of walks in $\cwalks_T$ from $0$ to $x$ and let $\cwalks(x) = \bigcup_{T \ge 0} \cwalks_T(x)$. We define the
\emph{two-point function}
\begin{equation}
G_x(\nu) = \mu(\cwalks(x)) \int_0^\infty e^{-\nu T} c_T(x) \; dT,
\end{equation}
where
\begin{equation}
c_T(x) = \int_{\cwalks_T(x)} e^{-H_T} \; d\lambda_T.
\end{equation}
% For $x \in \vertices$, let $\cwalks_T(x) \subset \cwalks_T$ denote the collection of walks $\omega \in \cwalks_T$ with $\omega_0 = a$ and $\omega_T = x$. The conditional measure $\mu^x_T = \mu_T(\cdot \mid \cwalks_T(x))$ is given by
% \begin{equation}
% \mu^x_T(d\omega) = \frac{\mu_T(d\omega) \1_{\cwalks_T(x)}(\omega)}{c_T(x)},
% \end{equation}
Note that
\begin{equation}
\chi(\nu) = \sum_{x\in\vertices} G_x(\nu),
\end{equation}
which is consistent with the analogous relation for spin systems. Later we will discuss the relationship between the two-point function for walks and spin systems.

The \emph{critical point} $\nu_c$ for a model of walks is defined
\begin{equation}
\nu_c = \inf (\nu : \chi(\nu) < \infty).
\end{equation}

% \todo{We really should not be defining terms for discrete- and continuous-time
% walks separately as (for example) $c_n$ in discrete-time is not the same as in
% continuous time. We should define discrete-time objects as the skeletons of
% continuous-time ones.}

% \begin{rk}
% In the discrete-time case, we let $z = e^{-\nu}$ and write
% \begin{equation}
% \mu(\omega)
%   =
% \frac{1 - e^{-\nu}}{\nu} z^{|\omega|} \mu_{|\omega|}(\omega)
% % \mu(f)
%   % =
% % \frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty z^n \int_{\dwalks_n} d\mu_n(\omega) \; f(\omega).
% \end{equation}
% with
% \begin{equation}
% \chi(\nu) = \frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty c_n z^n.
% \end{equation}
% Similarly,
% \begin{align}
% G_x(z) = \frac{1 - e^{-\nu}}{\nu} \sum_{n=0}^\infty c_n(x) z^n.
% \end{align}
% \end{rk}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Simple random walk}

% \todo{Restrict to $d_0$-regular graph (otherwise, generating function has $z_x$ instead of $z$).
% Work towards showing that generating function (for unkilled SRW) is inverse of:
% \begin{equation}
% 1 - z P = 1 - (z / d_0) J = (z / d_0) (m^2 + d_0 - J)
% \end{equation}
% with $m^2 = d_0 (1 - z) / z$.}

% The \emph{simple random walk} on $\graph$ is the Markov chain $X_n$ with transition matrix $P = d_0^{-1} J$. This induces the measure $\lambda_n$ on $\dwalks_n$ defined by
% \begin{equation}
% \lambda_n(\omega) = \Pr(X_k = \omega_k, \, k \le n \mid X_0 = 0).
% \end{equation}
% Then
% \begin{equation}
% c_n(0, x) = \Pr(X_n = x \mid X_0 = 0) = P^n_{0x}
% \end{equation}
% and
% \begin{equation}
% G_x = \sum_{n=0}^\infty (z P)^n_{0x} = (1 - z P)^{-1}_{0x},
% \end{equation}
% which converges for $|z| < 1$. Letting $m^2 = d_0 (1 - z) / z > 0$, we see that
% \begin{equation}
% 1 - z P = (z / d_0) (m^2 + L),
% \end{equation}
% so the two-point function is just the Green function for the massive Laplacian.

Let $\generator = -\lap$
Then $\generator$ is the generator of the $\vertices$-valued Markov process
$X = (X_t)_{t \ge 0}$ with transition probabilities
\begin{equation}
\Pr(X_t = y \mid X_0 = x) = (e^{-t \lap})_{xy},
\end{equation}
called the \emph{continuous-time simple random walk} on $\graph$.
We define the measure $\lambda_T$ on $\cwalks_T$ by
\begin{equation}
\lambda_T(d\omega) = \Pr(X_t = d\omega_t, \, t \le T \mid X_0 = 0).
\end{equation}
For the corresponding Gibbs measures (with $H_T \equiv 0$), we have
\begin{equation}
c_T(x) = (e^{-t \lap})_{0x},
  \quad
G_x = (\nu - \lap)^{-1}_{0x}
\end{equation}
for $\nu > 0$. Thus, the two-point function is the Green function for the
massive Laplacian.

\begin{example}
The discrete-time simple random walk is the Markov chain given by
$X_n = X_{\tau_n}$. Consider the discrete-time simple random walk on $\Zd$,
for which $X_n = X_0 + \sum_{i=1}^n Y_i$, where $Y_i$ are iid random variables
in $\{ e \in \Zd : |e| = 1 \}$ with $\Ex Y_i = 0$. Thus, $\Ex |X_n|^2 = n$ and
the central limit theorem implies that
$X_n / \sqrt n \Rightarrow \Ncal(0, 1)$. More generally, Donsker's invariance
principle states that a rescsaled version of $X$ converges to Brownian motion on $\Rd$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Weakly self-avoiding walk with self-attraction}

Define the \emph{local time} up to time $T$ of $\omega \in \cwalks$ at
$x \in \Zd$ by
\begin{equation}
\lt^x_T(\omega) = \int_0^T \1_{\omega(S)=x} \; dS.
\end{equation}
In the discrete-time case, $\lt^x_n$ is the number of times $\omega$ visits $x$
and is bounded by $n$. In the continuous-time case, $\lt^x_T$ is almost surely
finite for the continuous-time simple random walk.

We define the \emph{intersection local time}
\begin{equation}
\label{e:ITdef}
I_T(\omega) = \sum_{x\in\vertices} (\lt^x_T)^2
  =
\int_0^T \!\! \int_0^T \1_{\omega(S_1)=\omega(S_2)} \; dS_1 dS_2
\end{equation}
and the \emph{contact self-attraction}
\begin{equation}
\label{e:CTdef}
C_T(\omega) =
  \sum_{x \in \vertices} \sum_{y \sim x} \lt_T^x(\omega) \lt_T^y(\omega)
  = \int_0^T ds \int_0^T dt \; \1_{\omega_{s} \sim \omega_{t}}
\end{equation}
up to time $T$.
% Recall that we have set the inverse temperature equal to $1$.
Given a parameter $\gcc > 0$,
and $\gamma \in \R$, we define
\begin{equation}
\label{e:Udef-neg}
U_{\gcc,\gamma}(f)
=
\gcc \sum_{x\in\vertices} f_x^2
- \frac{\gamma}{2d}
\sum_{x\in\vertices} \sum_{y \sim x} f_x f_y
\end{equation}
for $f : \vertices \to \R$.
The \emph{weakly self-avoiding walk with self-attraction} (WSAW-SA) is defined via the Hamiltonian
\begin{equation}
H_T(\omega) = U_{\gcc,\gamma}(L_T(\omega)).
\end{equation}
We denote the canonical partition function by
\begin{equation}
c_T = c_T(\gcc, \gamma) = E_0 \left( e^{-\gcc I(T) + \gamma C(T)} \right),
\end{equation}
where $0 \in \vertices$ is fixed, and the susceptibility by
\begin{equation}
\chi(\gcc, \gamma, \nu) = \int_0^\infty c_T e^{-\nu T} \; dT.
\end{equation}

In the case $\gamma = 0$, the discrete-time version of this model is known as
the \emph{Domb-Joyce model}. In continuous-time, it is the
\emph{continuous-time weakly self-avoiding walk} (WSAW).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Critical behaviour}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Universality}

It is generally expected that critical exponents depend only on the very coarse structure of a model such as symmetry and ``tail properties'' (e.g.\ global geometry of the underlying graph). For instance, consider a ferromagnetic spin system on $\Zd$ and suppose that the single-spin measure
$d\lambda^0$ is invariant under Euclidean symmetries and whose interaction matrix $J$ is invariant under lattice symmetries ($J_{xy} = J_{Rx,Ry}$ for $R$ a rotation or reflection of $\Zd$) and is of finite-range ($J_{xy} = 0$ for $|x - y|$ sufficiently large). The exponents for such a system are expected only to depend on the dimensions $d$ of the underlying lattice and $n$ of the space of spins.

\todo{Discuss scaling limits}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The upper-critical dimension}

The critical exponents are conjectured to take on the following values:
\begin{equation}
\eta =
	\begin{cases}
	\frac{5 + n}{24},		& d = 2, \, n = 1 \\
	\approx 0.03,			& d = 3, \\
	0,						& d \ge 4
	\end{cases}
\qquad
\gamma =
	\begin{cases}
	\frac{43 + 13 n}{32},	& d = 2, \, n = 1 \\
	\approx 1,				& d = 3 \\
	1,						& d \ge 4
	\end{cases}
\end{equation}
and
\begin{equation}
\nubar =
	\begin{cases}
	\frac{3 + n}{4},		& d = 2, \, n = 1 \\
	\approx 0.6,			& d = 3 \\
	\frac{1}{2},			& d \ge 4
	\end{cases}
\end{equation}
with logarithmic corrections in $d = 4$.
Thus, it is expected that, when $d > 4$, the critical exponents cease to depend
on the dimension and $n$. In fact, they are expected to equal the exponents of
the corresponding non-interacting model, the Gaussian free field.

This phenomenon is known as \emph{mean-field behaviour} and dimension $4$ is
called the \emph{upper-critical dimension} for this class of models. The behaviour
of models is generally better understood above than below their upper-critical
dimension.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The renormalisation group}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definitions}

To unify our treatment of the two models, if $n \ge 1$ we let
\begin{equation}
\tau_x = \tfrac{1}{2} |\varphi_x|^2,
  \quad
\tau_{\Delta,x} = \tfrac{1}{2} \varphi_x \cdot (-\Delta \varphi)_x,
  \quad
|\nabla\tau_x|^2 = \sum_e (\nabla^e |\phi_x|^2)^2
\end{equation}
and define
\begin{equation}
U_{g,\gamma,\nu,N}
	=
\sum_{x\in\Lambda_N}
\Big(
	g \tau_x^2 + \nu \tau_x + z \tau_{\Delta,x} - \tfrac{1}{2 d} \gamma |\nabla\tau_x|^2
\Big).
\end{equation}
\todo{Define $\Ncal = \Ncal(n)$.}
Then for $F \in \Ncal$, we let
\begin{equation}
\langle F \rangle_{g,\gamma,\nu,N}
	=
\begin{cases}
\int F e^{-U_{g,\gamma,\nu,N}},								& n = 0 \\
\frac{1}{Z_{g,\gamma,\nu,N}}
	\int F(\varphi) e^{-U_{g,\gamma,\nu,N}} \; d\varphi,	& n \ge 1.
\end{cases}
\end{equation}
The two-point function is defined by
\begin{equation}
G_x(g, \gamma, \nu; n) = \lim_{N\to\infty} G_{x,N}(g, \gamma, \nu; n),
\end{equation}
where
\begin{equation}
G_{x,N}(g, \gamma, \nu; n)
	=
\begin{cases}
\langle \phib_0 \phi_x \rangle_{g,\gamma,\nu,N},			& n = 0 \\
\langle \varphi_0 \cdot \varphi_x \rangle_{g,\gamma,\nu,N}	& n \ge 1.
\end{cases}
\end{equation}
The susceptibility and finite-order correlation length are defined as before.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

The main result is stated below.
\todo{Discuss existence of $\nu_c$}

\begin{theorem} \label{thm:suscept}
  Let $d = 4$ and $n \ge 0$. For $L$ sufficiently large (depending on $n$),
  there exist $\gcc_* > 0$
  and a positive function $\gamma_* : (0, \gcc_*) \to \R$
  such that whenever $0 < \gcc < \gcc_*$ and $|\gamma| < \gamma_*(\gcc)$,
  there are constants $A_{\gcc,\gamma,n}$ and $B_{\gcc,\gamma,n}$ such that the following hold:

  \smallskip\noindent
  (i)
  The critical two-point function decays as
  \begin{equation}
    G_x(\gcc,\gamma,\nu_c; n)
        =
    A_{\gcc,\gamma} |x|^{-2} (1 + O((\log |x|)^{-1}))
        \quad
    \text{as $|x|\to\infty$},
  \end{equation}
  with $A_{\gcc,\gamma} = (4 \pi)^{-2} (1 + O(\gcc))$ as $\gcc \downarrow 0$.

  \smallskip\noindent
  (ii)
  The susceptibility diverges as
  \begin{equation} \label{e:chieps-asympt}
    \chi(\gcc, \gamma, \nu_c + \varepsilon; n)
      \sim B_{\gcc,\gamma,n} \varepsilon^{-1} (\log \varepsilon^{-1})^{(n+2)/(n+8)},
    \quad \varepsilon\downarrow 0
  \end{equation}
  with $B_{\gcc,\gamma,n} = ((n + 8) \gcc / 16\pi^2)^{(n+2)/(n+8)} (1 + O(\gcc))$
  as $\gcc \downarrow 0$.

  \smallskip\noindent
  (iii) For any $p >0$, if $L$ is chosen large and $\gcc_*$ small both depending on $p$,
  then the correlation length of order $p$ diverges as
  \begin{equation} \label{e:xieps-asympt}
    \xi_p(\gcc, \gamma, \nu_c + \varepsilon; n)
     \sim B_{\gcc,\gamma,n}^{1/2} {\sf c}_p \varepsilon^{-1/2} (\log \varepsilon^{-1})^{(n+2)/2(n+8)},
    \quad \varepsilon\downarrow 0
  \end{equation}
  with
  \begin{equation}
  {\sf c}_p^p = \int_{\R^4} |x|^p (-\Delta_{\R^4} + 1)^{-1}_{0x} \; dx.
  \end{equation}
\end{theorem}

The $\gamma = 0$ cases of (i) and (ii) were proved by Bauerschmidt, Brydges, and
Slade. The $n > 0$ case with $\gamma \ne 0$ is a new result in this thesis. We
will only discuss the proof of the $\gamma \ge 0$ case, which is of primary
interest. The proof of the $\gamma < 0$ case with $n = 0$ can be found in
\REF and the extension to $n > 0$ is straightforward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Relations between models}

There are a number of close relationships between models of walks and ferromagnetic spin
systems given by a Gibbs measure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The SRW and DGFF}

\todo{See candidacy report or preliminary version of it. See also misc notes}

For the discrete-time simple random walk, the canonical partition function $c_n$,
which is the number of such walks, is just $(2 d)^n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Kac-Siegert representation}

\todo{See notes on this}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Self-avoiding walk representations}

\todo{High-temperature expansion of spin system (can be used later to motivate polymer
expansion) to get loop models, De Gennes $n\downarrow0$ limit, McKane/Parisi-Sourlas
and supersymmetry, Grassmann integration and BIS representation (sufficiently general
for WSAW-SA)}